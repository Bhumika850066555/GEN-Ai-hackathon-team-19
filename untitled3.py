# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10JQ9byIat4eDyLrIkCgTlplQARjpDe-w
"""

pip install transformers

import json

from google.colab import files

# Upload the JSON file
uploaded = files.upload()

# Get the file path
file_path = next(iter(uploaded))

# Open and read the JSON file
with open(file_path, 'r') as file:
    data = json.load(file)

def preprocess_squad_dataset(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        squad_data = json.load(f)["data"]

    processed_data = []

    for article in squad_data:
        for paragraph in article["paragraphs"]:
            context = paragraph["context"]

            for qa in paragraph["qas"]:
                question = qa["question"]
                answer_texts = [answer["text"] for answer in qa["answers"]]

                processed_data.append({
                    "context": context,
                    "question": question,
                    "answers": answer_texts
                })

    return processed_data
dataset = preprocess_squad_dataset(file_path)

from transformers import GPT2Tokenizer

# Load the tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2-medium")

# Tokenize the dataset
def tokenize_dataset(dataset):
    tokenized_dataset = []
    for example in dataset:
        context = example["context"]
        question = example["question"]
        answers = example["answers"]

        # Tokenize the context
        tokenized_context = tokenizer.encode(context, add_special_tokens=False)

        # Tokenize the question
        tokenized_question = tokenizer.encode(question, add_special_tokens=False)

        # Tokenize the answers
        tokenized_answers = []
        for answer in answers:
            tokenized_answer = tokenizer.encode(answer, add_special_tokens=False)
            tokenized_answers.append(tokenized_answer)

        tokenized_example = {
            "context": tokenized_context,
            "question": tokenized_question,
            "answers": tokenized_answers
        }

        tokenized_dataset.append(tokenized_example)

    return tokenized_dataset

# Example usage
tokenized_dataset = tokenize_dataset(dataset)

def prepare_input_output_pairs(tokenized_dataset):
    input_output_pairs = []

    for example in tokenized_dataset:
        context = example["context"]
        question = example["question"]
        answers = example["answers"]

        for answer in answers:
            input_ids = context + question + answer
            output_ids = answer

            input_output_pair = {
                "input_ids": input_ids,
                "output_ids": output_ids
            }

            input_output_pairs.append(input_output_pair)

    return input_output_pairs

# Example usage
input_output_pairs = prepare_input_output_pairs(tokenized_dataset)

import torch
from transformers import GPT2Tokenizer

# Load the tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2-medium")
tokenizer.add_special_tokens({'pad_token': '[PAD]'})

# Encode the dataset
def encode_dataset(input_output_pairs):
    encoded_dataset = []

    for pair in input_output_pairs:
        input_ids = pair["input_ids"]
        output_ids = pair["output_ids"]

        encoded_input = tokenizer.encode_plus(
            input_ids,
            add_special_tokens=True,
            padding="max_length",
            max_length=512,  # Adjust max_length as per your requirements
            truncation=True,
            return_tensors="pt"
        )

        encoded_output = tokenizer.encode_plus(
            output_ids,
            add_special_tokens=True,
            padding="max_length",
            max_length=32,  # Adjust max_length as per your requirements
            truncation=True,
            return_tensors="pt"
        )

        encoded_pair = {
            "input_ids": encoded_input["input_ids"].squeeze(),
            "attention_mask": encoded_input["attention_mask"].squeeze(),
            "labels": encoded_output["input_ids"].squeeze()
        }

        encoded_dataset.append(encoded_pair)

    return encoded_dataset

# Example usage
e_dataset = encode_dataset(input_output_pairs)
# truncated_data = e_dataset[:1023]
# e_dataset = truncated_data

from google.colab import drive

drive.mount('/content/drive')

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2LMHeadModel,  AdamW
CUDA_LAUNCH_BLOCKING=1

class SquadDataset(Dataset):
    def __init__(self, encoded_dataset, tokenizer):
        self.encoded_dataset = encoded_dataset
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.encoded_dataset)

    def __getitem__(self, idx):
        item = self.encoded_dataset[idx]

        input_ids = item["input_ids"]
        attention_mask = item["attention_mask"]
        labels = item["labels"]

        # Define the maximum sequence length and chunk size
        max_length = 1024
        chunk_size = 512

        # Divide the input into smaller chunks
        chunks = [input_ids[i:i+chunk_size] for i in range(0, len(input_ids), chunk_size)]
        chunk_attention_masks = [attention_mask[i:i+chunk_size] for i in range(0, len(attention_mask), chunk_size)]
        chunk_labels = [labels[i:i+chunk_size] for i in range(0, len(labels), chunk_size)]

        processed_chunks = []
        for chunk, chunk_attention_mask, chunk_label in zip(chunks, chunk_attention_masks, chunk_labels):
        # Pad or truncate the chunk to the maximum sequence length
            chunk = torch.nn.functional.pad(chunk, (0, max_length - len(chunk)))
            chunk_attention_mask = torch.nn.functional.pad(chunk_attention_mask, (0, max_length - len(chunk_attention_mask)))
            chunk_label = torch.nn.functional.pad(chunk_label, (0, max_length - len(chunk_label)))

            processed_chunks.append({
                "input_ids": chunk,
                "attention_mask": chunk_attention_mask,
                "labels": chunk_label
            })

        return processed_chunks



# Load the pre-trained GPT-2 model and tokenizer
model_name = "gpt2-medium"
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# Set the maximum sequence length of the model
model.config.max_position_embeddings = 10240

# Prepare your encoded dataset
encoded_dataset = e_dataset  # Replace with your encoded dataset

max_length = 512

# Create a data loader
batch_size = 1
data_loader = DataLoader(
    SquadDataset(encoded_dataset, tokenizer),
    batch_size=batch_size,
    shuffle=True
)

# Define your training configuration
device = device = torch.device("cpu")
model.to(device)
optimizer = AdamW(model.parameters(), lr=1e-5)
num_epochs = 3

# Fine-tuning loop
# Fine-tuning loop
model.train()
for epoch in range(num_epochs):
    total_loss = 0
    for chunks in data_loader:
        batch_loss = 0
        for chunk in chunks:
            input_ids = chunk["input_ids"].to(device)
            attention_mask = chunk["attention_mask"].to(device)
            labels = chunk["labels"].to(device)

            optimizer.zero_grad()

            # outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            # loss = outputs.loss
            # loss.backward()

            optimizer.step()

    #         batch_loss += loss.item()

    #     total_loss += batch_loss / len(chunks)

    # avg_loss = total_loss / len(data_loader)
    # print(f"Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f}")


# Save the fine-tuned model and tokenizer
model.save_pretrained("drive/My Drive/dataset/model")
tokenizer.save_pretrained("drive/My Drive/dataset/model")

from transformers import GPT2LMHeadModel, GPT2Tokenizer
from transformers import AutoModelForQuestionAnswering
from transformers import AutoTokenizer

model_dir = "drive/My Drive/dataset/model"


model = GPT2LMHeadModel.from_pretrained(model_dir)
tokenizer = GPT2Tokenizer.from_pretrained(model_dir)

model.eval()

prompt = "Your prompt goes here"
input_ids = tokenizer.encode(prompt, return_tensors="pt")

max_length = 100  # Set the maximum length of the generated output
output = model.generate(input_ids, max_length=max_length)


output_text = tokenizer.decode(output[0], skip_special_tokens=True)

print(output_text)

# max_output_length = 1000

# output_tokens = model.generate(
#     input_tokens,
#     max_length=input_tokens.size(1) + 50,
#     pad_token_id=tokenizer.eos_token_id,
#     do_sample=True,
#     attention_mask=attention_mask
# )
# output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)

# print(output_text)